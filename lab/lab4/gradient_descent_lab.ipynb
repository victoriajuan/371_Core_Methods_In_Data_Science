{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "\n",
    "INFO 371 Spring 2018\n",
    "\n",
    "Lab created by: Christie Gan\n",
    "\n",
    "Deadline: 4/25/18 11:59PM\n",
    "\n",
    "Last Edit: 4/21/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most commonly used machine learning algorithms, gradient descent uses gradient/derivative to find the values of function parameters. It changes the parameters iteratively to minimize a given function to its local minimum. In this lab, you'll be implementing the algorithm for function f(x) = x^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Pick an initial value of the parameter x<sub>0</sub> (don't choose 0), which will be your starting value. Pick the number of iterations you want the algorithm to go through. Set these two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) The function given is f(x) = x^2, and the hyper-parameter p (learning rate) is 0.1. What is the gradient of f(x) = x^2? When the gradient is 0, we get the minimum. What would be the correct location of the minimum of the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the gradient of the function, you just need to find the vector of partial derivatives. For a 1D function like x^2, the vector is just a scalar value. The gradient is commonly denoted as â–½f(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.) What is the gradient of f(x) = x^2 at x<sub>0</sub>? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.) How would the algorithm find the minimum of the plot, given the infomation above? Find an equation illustrating the algorithm. Given the equation, compute x<sub>1</sub>. You can do this on paper or by code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.) Pick a stopping critieria of when the gradient is very small (when should the algorithm stop?) You will use this in an if condition later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.) Implement the 1D gradient descent algorithm based on the information above. You can either write just the while loop, or put it into a function. Print out the min location (x) and min value (f(x)), and how many loops it took to become optimized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning uses this algorithm to optimize the cost function so that the errors are minimized. In our example, we are using a general function, f(x) = x^2, to illustrate the algorithm. In machine learning, this f(x) will be a cost function. For example, for simple linear regression, the cost function will be mostly likely be mean squared errors cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
